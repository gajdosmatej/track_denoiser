# track_denoiser
This repository forms the codebase for the paper M. Gajdo≈°, H.N. da Luz, G.G.A. de Souza, M. Bregant, *TPC track denoising and recognition using convolutional neural networks*, CPC (2025), https://doi.org/10.1016/j.cpc.2025.109608.

## Setup on Linux
1. **Create Python virtual environment:** `python3 -m venv .venv` (or possibly just `python` instead of `python3`, depending on the Linux distribution)
2. **Activate the virtual environment:** `source .venv/bin/activate`. This step needs to be applied each time when working with this project.
3. **Install dependencies:** `pip install -r dependencies.txt`.
4. **Create data folders:** If you have access to UTEF-X17 MetaCentrum storage, use the bash script `init_structure_data.sh`, which will also download the measured cosmics data. Otherwise, use the script `init_structure.sh` to construct the correct data folders tree.

## Experimental data
The experimental data are available upon request. Each event is stored in its own `.txt` file, having rows of the form $(x,y,t,E)$, where $E$ stands for the amplitude; coordinates not present in any row are thought as describing zero amplitude. `data/x17/gauge_backgrounds` contain measured events with tracks removed, so that only noisy patterns are present.

## Analysis using the paper neural network
The Jupyter notebook `analysis.ipynb` contains various views, computations and plots regarding the data and neural network results. Most of the plots from the paper should be possible to recreate in this notebook, provided the experimental data are available.

## Synthetised data
For neural network training, synthetised data are being used, generated by `track_generator.py`. Those are stored in `data/simulated/` (denoising/semantic segmentation) and `data/labeling` (track recognition/labeling) either in `clean/` or `noisy/` subdirectories. The generated data are stored in numpy files `<INDEX>.npy` with the integer `INDEX` matching the corresponding clean and noisy files. Each such numpy file contain 5000 generated events, which can be easily adjusted in `track_generator.py`.

The `track_generator.py` admits several flags. To see the list of commands, use `-h`. To specify the number of *batches* (=files, each containing 5000 events), use `-n <INT>`. To specify whether the data are aimed for denoising or track recognition, use `-l <0/1>` (0 for denoising, 1 for recognition). To specify the type of noise employed, use `-m <0/1/2>`, where 0 indicates only synthetic noise, 1 indicates only measured noise masks and 2 indicates mix of both approaches. The full command needs to specify all of these flags (excluding `-h`), e.g.
```
python track_generator.py -n 10 -l 0 -m 2
```

In order to use measured noise masks, the experimental data in `data/x17/gauge_backgrounds/` need to be available. If this is not the case, use `-m 0`.

## Neural network training
It is best to perform the neural network training on a computer with GPU supported by TensorFlow. For this purpose, MetaCentrum cluster can be used, the procedure is described in the following. On the other hand, local training is straightforward (just Keras training using a user-defined NN architecture and training data created by `track_generator.py`). 

The only thing worth mentioning even for the local training is the loading of the training data. For that, the class `DataLoader` (located in `classes/dataLoaderClass.py`) exists. An instance of this class is created as 
```
data_loader = DataLoader(path_to_root_data_dir),
```
where `path_to_root_data_dir` is locally just `data/` (at least if the directories follow the structure created by `init_structure.sh`). On cluster, when data are mounted e.g. to `/training_data/`, the path would be that exact `/training_data/`.

`DataLoader` instance then lets you create TensorFlow dataset for direct training by calling its function `DataLoader.getDataset(low, high, batch_size)`. The variables `low` and `high` define the range of file names which shall be used (the training synthetised files are named `<INDEX>.npy`, so `low` and `high` specify which of these `<INDEX>` should be used during training). `batch_size` defines the size of batches for the training itself, i.e. it is independent of the number of entries in each data file (which is also called batch). It defines the number of training entries which are being evaluated concurrently. Each batch updates the weights precisely once, even though multiple events are inside of the batch.

### MetaCentrum cluster
Firstly, you need to have training data stored somewhere at the MetaCentrum cluster. For that, you can use interactive job on the cluster, e.g.
```
qsub -I -l select=1:mem=32gb:scratch_local=20gb:ncpus=8 -l walltime=1:00:00
```
Then: 
1. Go to scratchdir by `cd $SCRATCHDIR`. 
2. Get there also `track_generator.py` and `classes/dataLoaderClass.py`, for example by copying it from your local installation to some available `/storage/` at the cluster, and then copying it from there to the scratchdir.
3. Create the standard directory structure in the scratchdir (you can also use the script `init_structure.sh`). 
4. Load a MetaCentrum Python module, e.g.
	```
	module load python36-modules/gcc
	```
	Those can be listed by 
	```
	module avail "python*modules"/
	```
5. Run the `track_generator.py` script as described in previous text.
6. Move the `data/` directory to an available `/storage/`.

For the training itself, use GPU interactive job, e.g.
```
qsub -I -l select=1:mem=32gb:scratch_local=20gb:ngpus=1:gpu_cap=compute_70 -l walltime=1:00:00 -q gpu
```
The flags `-q gpu` and `ngpus=` need to be specified in order to use GPU. However, it still sometimes happens that even with this command, the training script will not be able to use the cluster's GPU. It is suspected that this is due to a particular combination of the available GPU and the installed TensorFlow. Connecting to a different computational node by starting a new interactive job usually helps.

After the interactive job starts: 
1. Connect to the scratchdir by `cd $SCRATCHDIR`.
2. Copy there `classes/` and your training Python script.
3. Start Singularity, e.g.
	```
	singularity shell -B $SCRATCHDIR:/scratchdir -B /storage/<YOUR PATH TO DATA DIR>:/data --nv /cvmfs/singularity.metacentrum.cz/NGC/TensorFlow\:24.04-tf2-py3.SIF
	```
	This binds your scratchdir to `/scratchdir/`, datadir to `/data/` and creates an environment with TensorFlow and Keras available. To list all the different available TensorFlow environments, use
	```
	ls /cvmfs/singularity.metacentrum.cz/NGC/
	```
4. Go to `/scratchdir`.
5. Run the training script. Note that data need to be loaded from `/data/`.
6. Exit Singularity (Ctrl+D). If you saved any trained model, move it from the scratchdir to another storage before exiting the interactive job.

## Pretrained neural network
The pretrained neural network, whose architecture is described in the paper, is stored in several formats in `models/3D/`: 
- The folder `M1/` is the original saved version using the TensorFlow's `SavedModel` format, which is not supported anymore in Keras 3. 
- The files `M1.h5` and `M1.keras` can be both used to load the model even in Keras 3.
- The file `M1.weights.h5` contains the exported weights of the neural network. In order to load these, the network's architecture needs to be specified in the code, but it can be conveniently retrieved using `getPaperModel()` in `classes/modelWrapperClass.py`.
